<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Scalability | mojobojo.com]]></title>
  <link href="http://boj.github.io/blog/categories/scalability/atom.xml" rel="self"/>
  <link href="http://boj.github.io/"/>
  <updated>2013-05-20T17:14:50+09:00</updated>
  <id>http://boj.github.io/</id>
  <author>
    <name><![CDATA[Brian "bojo" Jones]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[RackSpace and iptables]]></title>
    <link href="http://boj.github.io/blog/2011/03/04/rackspace-iptables-batten-down-the-hatches/"/>
    <updated>2011-03-04T00:00:00+09:00</updated>
    <id>http://boj.github.io/blog/2011/03/04/rackspace-iptables-batten-down-the-hatches</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>This is a followup post to my last entry about <a href="http://boj.github.com/blog/2011/01/14/lvs-nginx-nodejs-mongodb-cluster-setup-on-rackspace/">LVS &ndash; Nginx &ndash; NodeJS &ndash; MongoDB &ndash; Cluster Setup on RackSpace</a>, and focuses on implementing iptables rules to lock down the cluster from both external and internal probing.  It&rsquo;s important to note that RackSpace&rsquo;s servers are wide open on all interfaces, and that other members in their internal network can sniff ports on your machine&rsquo;s internal interfaces.</p>

<p>I am by no means an iptables expert, and while I have used the tool for many reasons over the years it has never been with complete comprehension as to how it works beyond a superficial level.  Therefore, if anything in regards to my settings below seem to be off, strange, or makes no sense, please feel free to leave advice in the comments section below.  I am publishing this for myself as much as the next guy who needs a few hints about setting up iptables rules for things like LVS-TUN or RackSpace.</p>

<p>I will keep things simple and just breakdown my rule sets by their specific jobs.  I figure little to no explanation is needed.</p>

<!-- more -->


<h3>Note</h3>

<p>It&rsquo;s very useful when working on a network server to remember to run the following rule first before anything else so you don&rsquo;t lock yourself out of your server.  RackSpace has a handy console which you can use via their browser interface should you kill your ssh connection, so this is a tip to cut down on hassle more than anything else.</p>

<pre><code>iptables -P INPUT ACCEPT
</code></pre>

<p>My rules also include the command <em>/sbin/service iptables save</em> which is RHEL/CentOS specific and saves the configuration out for you.</p>

<h3>Admin Server Configuration</h3>

<pre><code>iptables -P INPUT ACCEPT
iptables -F
iptables -A INPUT -p tcp --dport 22 -j ACCEPT
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -p udp --dport 8646:8649 -j ACCEPT # gmond

iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

iptables -A INPUT -i lo -j ACCEPT

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

/sbin/service iptables save

iptables -L -v
</code></pre>

<p>The reason gmond runs on 4 ports will be explained in a later blog post about setting up Ganglia in unicast mode.</p>

<h3>Load Balancer Configuration</h3>

<p>First you must edit /etc/services and add the following line.</p>

<pre><code>vrrp            112/raw
</code></pre>

<p>Then add your rules.</p>

<pre><code>iptables -P INPUT ACCEPT
iptables -F
iptables -A INPUT -p tcp --dport 22 -s x.x.x.x -j ACCEPT # Only allow access from the admin server
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -p udp --dport 8649 -j ACCEPT # gmond
iptables -A INPUT -p tcp --dport 8649 -j ACCEPT # gmond

iptables -p vrrp -A INPUT -j ACCEPT
iptables -p vrrp -A OUTPUT -j ACCEPT

iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

iptables -A INPUT -i lo -j ACCEPT

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

/sbin/service iptables save

iptables -L -v
</code></pre>

<p>The vrrp commands allow your LVS machines to broadcast to each other so they can take over in case the other fails.</p>

<h3>Application Server Configuration</h3>

<pre><code>iptables -P INPUT ACCEPT
iptables -F
iptables -A INPUT -p tcp --dport 22 -s x.x.x.x -j ACCEPT # Only allow access from the admin server
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -p udp --dport 8649 -j ACCEPT # gmond
iptables -A INPUT -p tcp --dport 8649 -j ACCEPT # gmond

iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

iptables -A INPUT -i lo -j ACCEPT
iptables -A INPUT -i eth1 -j ACCEPT

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

/sbin/service iptables save

iptables -L -v
</code></pre>

<p>The eth1 rule is to allow LVS to connect to the realserver.  Without it your connection will just hang at the director level.</p>

<h3>Database Cluster Configuration</h3>

<pre><code>iptables -P INPUT ACCEPT
iptables -F
iptables -A INPUT -p tcp --dport 22 -s x.x.x.x -j ACCEPT # Only allow access from the admin server
iptables -A INPUT -p tcp --dport 27017 -j ACCEPT # mongodb
iptables -A INPUT -p tcp --dport 28017 -j ACCEPT # mongodb web stats
iptables -A INPUT -p udp --dport 8649 -j ACCEPT # gmond
iptables -A INPUT -p tcp --dport 8649 -j ACCEPT # gmond

iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

iptables -A INPUT -i lo -j ACCEPT

iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

/sbin/service iptables save

iptables -L -v
</code></pre>

<h2>Conclusion</h2>

<p>There&rsquo;s really not much different between the scripts except for a few token places specific to the server&rsquo;s services.  I will update this blog post if I discover better ways to tighten things down.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LVS - Nginx - NodeJS - MongoDB - Cluster Setup on RackSpace]]></title>
    <link href="http://boj.github.io/blog/2011/01/20/lvs-nginx-nodejs-mongodb-cluster-setup-on-rackspace/"/>
    <updated>2011-01-20T00:00:00+09:00</updated>
    <id>http://boj.github.io/blog/2011/01/20/lvs-nginx-nodejs-mongodb-cluster-setup-on-rackspace</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>This article will lay out the steps I used to build a high availability scalable social game server cluster on the RackSpace Cloud for the latest project I am working on at <a href="http://www.istpika.com/en">Istpika</a>.  The basic requirements were a simple HTTP based API which exposes a RESTful-like interface to the client, pioneer our first use of MongoDB as the backend database, a fault tolerant setup, and a service in North America.  The actual client is built on smartphones with the game logic written in JavaScript using a currently undisclosed SDK.  With that in mind, it seemed like building a full stack JavaScript system would make development a lot smoother for all parties involved, so I also decided to write the server logic in NodeJS, all of which ties in neatly with MongoDB&rsquo;s JavaScript system.</p>

<!-- more -->


<p>Ultimately the service selection boiled down to either Amazon EC2, or the RackSpace Cloud.  While I am admittedly biased towards RackSpace due to the fact that my favorite service <a href="https://github.com/blog/493-github-is-moving-to-rackspace">GitHub</a> runs on them and I run my personal servers on their cloud service, after a bit of research they were still clearly the winners in regards to flexibility and compute power.  I have no solid opinion in regards to Amazon EC2 since I have never directly used them, but large social game companies like Playfish seem to be <a href="http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html">using them without any problems</a>.  I did find an interesting <a href="http://www.thebitsource.com/featured-posts/rackspace-cloud-servers-versus-amazon-ec2-performance-analysis/">article</a> showing speed comparisons between the two services, with RackSpace coming out ahead on most benchmarks.  My only major concern at this point is the 16GB memory limit of the largest Cloud Servers for the databases, but am sure that if it becomes a problem MongoDB&rsquo;s sharding will come into play and save the day.</p>

<p>I will leave out most of the small configuration details in this blog and will be left as an exercise to the reader.</p>

<h2>Initial Setup</h2>

<p>There&rsquo;s a handful of things I like to do to make managing the servers and securing them easier.  The first step is to make an administration server which all SSH access to the servers will be routed through.  This allows us to shut down the public facing interfaces on most of the internal servers, disable external SSH access on servers with external interfaces, and create a barrier between them and the internet.  We also typically use this server for secondary services, such as remote logging, system monitoring, data mining tools, etc.</p>

<p>After creating a new server instance via the RackSpace control panel and logging in for the first time, it&rsquo;s recommended to create an &lsquo;access user&rsquo; and append your public SSH key to their .ssh/authorized_keys file, add them to the admin group and sudoers file, and turn off root login and password access to SSH.  After that create a SSH key for the root user, this will be used to allow direct login access to the other servers.</p>

<p>For each additional server that is brought up we copy the root SSH key over, and in the case of an internal only server disable the external interface that RackSpace automatically assigns a public IP to.  To make things easy I also edit the <em>/etc/hosts</em> file and add a simple name entry based on the server&rsquo;s function along with the internal IP of the new server (e.g. 10.0.01 admin).</p>

<h2>Load Balancers &ndash; LVS and Keepalived</h2>

<p>In order to get all of this going we need to set up some frontend load balancers which direct all traffic to the application servers behind them, and will failover in case one of the load balancers go down.  RackSpace has announced that they will eventually offer this as a service and <a href="http://www.rackspacecloud.com/blog/2010/11/09/announcing-cloud-load-balancing-private-beta/">that it is currently in private beta for those interested</a>, however due to time constraints I decided to implement my own solution.  I found a helpful RackSpace blog about <a href="http://www.rackspacecloud.com/blog/2010/09/22/installing-and-configuring-lvs-tun/">setting up LVS-TUN on their service</a> using pulse, but opted to use LVS-TUN + keepalived instead due to the fact that I&rsquo;ve had more experience with it up to this point.</p>

<p>This kind of setup is ideal because it allows you to maximize the throughput of all your servers (the connection comes in through the load balancers, but all traffic is handled by the application servers), and is very scalable in regards to geographical locations.</p>

<h3>Terminology</h3>

<p>These are some terms used in regards to LVS.</p>

<ul>
<li>Directors &ndash; The load balancers.</li>
<li>Realservers &ndash; The app servers which are being load balanced.</li>
<li>VIP &ndash; The virtual IP which floats between the directors.</li>
<li>RIP &ndash; Realserver IP.</li>
</ul>


<h3>Initial Preparation</h3>

<p>First setup two servers from the RackSpace control panel to act as your directors.  A server size of 256mb memory will be just fine since it is just routing traffic to your application servers behind it.</p>

<p>In order for load balancing to work we will need to request a shared IP.  You will need to open a support ticket and request an IP which they will assign to the two directors for you, as well as tell them which realservers will be replying back to the clients.  This is very simple and painless, just make sure to explain to them that the additional IP will be used for failover and there shouldn&rsquo;t be any problems.</p>

<p>It&rsquo;s also ideal to disable your iptables rules until everything has been properly debugged and is working.</p>

<h3>Directors</h3>

<p>The first step is to build two LVS-TUN configured directors.  The two fundamental packages we will need are <em>ipvsadm</em> and <em>keepalived</em>.  The keepalived configuration should be similar to the following, which assumes an HTTP service running on port 80.</p>

<pre><code>#
# /etc/keepalived/keepalived.conf
#

# global defs
global_defs {
    notification_email {
      someone@example.com
    }
    notification_email_from keepalived@director
    smtp_server 127.0.0.1
    smtp_connect_timeout 30
    router_id my_router
}

# vips app-master
vrrp_instance app_master {
    state MASTER # Set this to BACKUP on the secondary server
    interface eth0
    virtual_router_id 36 # I usually use the last octect of the internal IP here, but anything is OK
    priority 100
    nopreempt
    advert_int 1
    garp_master_delay 5
    smtp_alert
    virtual_ipaddress {
        w.x.y.z # The VIP address, requested via RackSpace ticket
    }
}

# master app
virtual_server w.x.y.z 80 { # VIP address
  delay_loop  1
  lvs_sched   wlc
  lvs_method  TUN
  protocol    TCP
  real_server a.b.c.d 80 { # Internal IP of our realserver
    weight 10
    TCP_CHECK {
      connect_port 80
      connect_timeout 15
    }
  }
  # Continue to add entries for each realserver
}
</code></pre>

<p>You can erase the entry for the eth0:1 device which the RackSpace techs setup, since this will be automatically configured for you via keepalived.</p>

<p>You will also want to edit <em>/etc/sysctl.conf</em> on the directors and add the following.  This setting allows the VIP to be shared between two machines.</p>

<pre><code>net.ipv4.ip_nonlocal_bind = 1
</code></pre>

<p>Then run the <em>sysctl -p</em> command so the settings take effect.</p>

<h3>Realservers</h3>

<p>Next you will need to configure the realservers and setup a tunnel device.  This allows the traffic to be forwarded from the director to the realserver, and the realserver to reply directly back to the client.  On CentOS 5.5 you would edit <em>/etc/sysconfig/network-scripts/ifcfg-tunl0</em> and add the following.</p>

<pre><code>DEVICE=tunl0
TYPE=ipip
IPADDR=w.x.y.z # The VIP assigned to you by RackSpace
NETMASK=255.255.255.255
BROADCAST=w.x.y.z # Broadcast address is the same as the VIP
ONBOOT=yes
</code></pre>

<p>Then bring it up with the <em>ifup tunl0</em> command.</p>

<p>Next, in order to get around what is known as the <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.arp_problem.html">ARP Problem</a> we also need to edit the realserver&rsquo;s <em>/etc/sysctl.conf</em>.</p>

<p>Borrowed from the RackSpace LVS-TUN blog, it should look similar to this.</p>

<pre><code>net.ipv4.ip_forward = 0
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.ipv4.tcp_syncookies = 1
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296
net.ipv4.conf.lo.arp_ignore = 1
net.ipv4.conf.lo.arp_announce = 2
net.ipv4.conf.tunl0.arp_ignore = 1
net.ipv4.conf.tunl0.arp_announce = 2
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.all.arp_announce = 2
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_max_syn_backlog = 2048
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.lo.rp_filter = 0
net.ipv4.conf.eth0.rp_filter = 0
net.ipv4.conf.eth1.rp_filter = 0
net.ipv4.conf.tunl0.rp_filter = 0
</code></pre>

<p>Then set with <em>sysctl -p</em>.</p>

<h2>Application Servers &ndash; Node, Nginx, and Monit</h2>

<p><em>UPDATE &ndash; 2011-03-4:</em> The Mongoose module had just reached it&rsquo;s 1.0 release, but still had a lot of incomplete features and bugs and had gone through 10 minor revisions within a week.  I&rsquo;m still a huge fan of NodeJS and Mongoose, but had to set it aside this time around.  For posterity sake I will keep the following section intact, but want to point out that in it&rsquo;s place I setup Ruby Enterprise Edition + Passenger on Nginx and wrote the server logic using the super lightweight Ramaze framework plus MongoMapper.  I have to admit the Ruby fan in me had no regrets about going this direction.</p>

<p>The main goal here is to get multiple instances of our NodeJS server running on a single server instance, and proxying them behind an Nginx server.  The main reason for this is because RackSpace servers have 4 CPUs, and we would like to make use of all of them.  I am also not 100% confident in running NodeJS as a stable standalone process, so we want another layer in front of it in the case of failure.</p>

<h3>NodeJS</h3>

<p>First of all we setup our NodeJS server.</p>

<pre><code>var http = require('http');
http.createServer(function (req, res) {
  res.writeHead(200, {'Content-Type': 'text/plain'});
  res.end('Hello World\n');
}).listen(8001);
console.log('Server running on port 8001');
</code></pre>

<p>I also have some database code written with the <a href="https://github.com/LearnBoost/mongoose">Mongoose</a> package to test connectivity to the database.  This becomes relevant after the database servers are brought up, but will be left as an exercise to the reader.</p>

<h3>Monit</h3>

<p>I then use Monit to make sure that the NodeJS server processes continue to run even after a critical failure.  There are a few methods with which to set this up, and I am still experimenting with my settings to get it working correctly.  Currently my config looks like the following.</p>

<pre><code># /etc/monit.d/my_project

check host my_project01 with address 127.0.0.1 
    start program = "/usr/local/bin/node /path/to/project/server1.js"
        as uid nobody and gid nobody
    stop program = "/usr/bin/pkill -f '/usr/local/bin/node /path/to/project/server1.js'"
    if failed port 8001 protocol HTTP
        request /
        with timeout 10 seconds
        then restart

check host my_project02 with address 127.0.0.1
    start program = "/usr/local/bin/node /path/to/project/server2.js"
        as uid nobody and gid nobody
    stop program = "/usr/bin/pkill -f '/usr/local/bin/node /path/to/project/server2.js'"
    if failed port 8002 protocol HTTP
        request /
        with timeout 10 seconds
        then restart

# ... etc.
</code></pre>

<p>My eventual goal here is to write the server code with an optional port parameter as a command line option.</p>

<p>I looked into using <a href="http://blog.nodejitsu.com/keep-a-nodejs-server-up-with-forever">forever</a> to keep the NodeJS processes running, but that is just a CLI tool and not a long running server process which comes back up after a server reset.</p>

<h3>Nginx</h3>

<p>Finally we setup Nginx as our proxy balancer.  Apache is also a viable option, but in the end boils down to preference.</p>

<pre><code># /etc/nginx/nginx.conf

worker_processes  1;

events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;

    keepalive_timeout  65;

    upstream my_project {
        server 127.0.0.1:8001;
        server 127.0.0.1:8002;
    }

    server {
        listen       80;
        server_name  my_project;

        location / {
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $http_host;
            proxy_set_header X-NginX-Proxy true;
            proxy_pass http://my_project/;
            proxy_redirect off;
        }

    }
}
</code></pre>

<p>This is a very basic config, and should probably be expanded upon.</p>

<h3>ipvsadm</h3>

<p>We should now be be able to verify our connection externally via our VIP, and see traffic coming in through the director.</p>

<pre><code>[root@director01 ~]# watch ipvsadm -Ln

Every 2.0s: ipvsadm -Ln                                 Thu Jan 20 09:06:40 2011

IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  VIP:80 wlc
  -&gt; RIP:80                       Tunnel  10     1          0
</code></pre>

<p>If this step doesn&rsquo;t work, verify that you can access each realserver from it&rsquo;s public IP.  If that fails and your iptables rules are off, double check that the RackSpace techs have setup your realserver so I can be accessed using the assigned VIP.</p>

<h2>Database Servers &ndash; MongoDB Replica Sets (v1.6)</h2>

<p>As of MongoDB v1.6 <a href="http://www.mongodb.org/display/DOCS/Replica+Sets">Replica Sets</a> are now stable and production ready.  The basic idea is to allow for master/slave replication, failover and recovery, and in the case of master failure automatic election of a new master node.  Up to a total of 7 servers can automatically be added in case the database load needs to be scaled, and is as simple as typing a single command.</p>

<p>Ideally you want to start with three servers running, one master, one slave, and one backup which we turn off and backup periodically.</p>

<p>For those of you that use CentOS or Fedora, <a href="http://www.mongodb.org/display/DOCS/CentOS+and+Fedora+Packages">packages are available</a> to install.</p>

<p>The only setting you need to add to the default config is your replication set name.</p>

<pre><code># /etc/mongod.conf

replSet = my_project
</code></pre>

<p>After that bring up all three database instances, and then log into any node with the <em>mongo</em> command line tool.  We will now tell the cluster which servers to bring into the replication set.  You may want to watch the log output on all three servers as well with <em>tail -f /var/log/mongo/mongod.log</em>, I admit it was pretty awesome watching them negotiate who was the master and slaves for the first time.</p>

<pre><code>config = {
  _id: 'my_project', 
  members: [ 
    {_id: 0, host: 'db-01'},
    {_id: 1, host: 'db-02'},
    {_id: 2, host: 'db-03'}
  ] 
}
rs.initiate(config)
</code></pre>

<p>If you ever need to attach additional nodes, you can simply do the following.</p>

<pre><code>rs.add('db-04')
</code></pre>

<p>The mongo client drivers automatically learn new server addresses from the seed addresses they were set with, so no client updates are required when expanding your database cluster.</p>

<h2>Final Notes</h2>

<h3>Considerations</h3>

<p>Assuming all has gone well up to this point you will then want to harden your servers with <em>iptables</em>, disable any services that you don&rsquo;t need running, and batten down the hatches.</p>

<p>You will want to setup a database backup system and schedule, as well as logging so you can track down what your cluster is doing.</p>

<p>I also like to setup Nagios and Ganglia to monitor my services and see what kind of usage they are getting, although there are other tools and services which work just as well.</p>
]]></content>
  </entry>
  
</feed>
